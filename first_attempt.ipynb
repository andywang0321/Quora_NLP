{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306117</th>\n",
       "      <td>ffffcc4e2331aaf1e41e</td>\n",
       "      <td>What other technical skills do you need as a c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306118</th>\n",
       "      <td>ffffd431801e5a2f4861</td>\n",
       "      <td>Does MS in ECE have good job prospects in USA ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306119</th>\n",
       "      <td>ffffd48fb36b63db010c</td>\n",
       "      <td>Is foam insulation toxic?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306120</th>\n",
       "      <td>ffffec519fa37cf60c78</td>\n",
       "      <td>How can one start a research project based on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306121</th>\n",
       "      <td>ffffed09fedb5088744a</td>\n",
       "      <td>Who wins in a battle between a Wolverine and a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1306122 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "0        00002165364db923c7e6   \n",
       "1        000032939017120e6e44   \n",
       "2        0000412ca6e4628ce2cf   \n",
       "3        000042bf85aa498cd78e   \n",
       "4        0000455dfa3e01eae3af   \n",
       "...                       ...   \n",
       "1306117  ffffcc4e2331aaf1e41e   \n",
       "1306118  ffffd431801e5a2f4861   \n",
       "1306119  ffffd48fb36b63db010c   \n",
       "1306120  ffffec519fa37cf60c78   \n",
       "1306121  ffffed09fedb5088744a   \n",
       "\n",
       "                                             question_text  target  \n",
       "0        How did Quebec nationalists see their province...       0  \n",
       "1        Do you have an adopted dog, how would you enco...       0  \n",
       "2        Why does velocity affect time? Does velocity a...       0  \n",
       "3        How did Otto von Guericke used the Magdeburg h...       0  \n",
       "4        Can I convert montra helicon D to a mountain b...       0  \n",
       "...                                                    ...     ...  \n",
       "1306117  What other technical skills do you need as a c...       0  \n",
       "1306118  Does MS in ECE have good job prospects in USA ...       0  \n",
       "1306119                          Is foam insulation toxic?       0  \n",
       "1306120  How can one start a research project based on ...       0  \n",
       "1306121  Who wins in a battle between a Wolverine and a...       0  \n",
       "\n",
       "[1306122 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"./data/train.csv\"\n",
    "data_csv = pd.read_csv(data_dir)\n",
    "data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = data_csv[:1_000_000]\n",
    "test_csv = data_csv[1_000_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:00, 36221.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n",
      "Vocab length: 2196016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_dir = \"./data/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "\n",
    "glove = {}\n",
    "vocab = set()\n",
    "\n",
    "with open(glove_dir, encoding = \"utf8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        values = line.split(\" \")\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype = 'float32')\n",
    "        glove[word] = vector\n",
    "        vocab.add(word)\n",
    "\n",
    "print('Found %s word vectors.' % len(glove))\n",
    "print(f'Vocab length: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why', 'can', 'some', 'birds', 'fly', 'but', 'others', 'cant']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_list(text, vocab = vocab):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('/', ' ').split()\n",
    "    return [word for word in text if word in vocab]\n",
    "\n",
    "test_str = \"Why can some birds fly, but others can't?\"\n",
    "\n",
    "text_to_list(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text_list):\n",
    "    # TODO: tokenizer shouldn't embed text.\n",
    "    # simply return index of word in vocab.\n",
    "    return np.array([glove.get(word) for word in text_list])\n",
    "\n",
    "tokenizer(text_to_list(test_str)).shape # np.array of (num_words X 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x : tokenizer(text_to_list(x))\n",
    "label_pipeline = lambda x : x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        \n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.float32)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        offsets.append(processed_text.size(0)) # num_words\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype = torch.int16)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim = 0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_csv = train_csv[:50]\n",
    "mini_iter = ((row['target'], row['question_text'])\n",
    "             for _, row in mini_csv.drop(columns = [\"qid\"]).iterrows())\n",
    "mini_dataset = to_map_style_dataset(mini_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  13,  29,  39,  48,  63,  73,  91, 105, 123, 166, 172, 178, 190,\n",
       "        209, 234, 245, 260, 268, 289, 298, 311, 328, 339, 366, 375, 385, 400,\n",
       "        408, 420, 430, 445, 457, 469, 491, 497, 522, 535, 540, 553, 560, 573,\n",
       "        583, 592, 603, 619, 642, 658, 688, 729], device='mps:0')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c = collate_batch(mini_dataset)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 300])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[c[2]:c[3]].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(QuestionClassifier, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse = False)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "vocab_size = len(vocab)\n",
    "emsize = 300\n",
    "model = QuestionClassifier(vocab_size, emsize, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictied_label = model(text, offsets)\n",
    "        loss = criterion(predictied_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predictied_label.argmax(1) == label()).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += lable.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_map_style_dataset(iter_data):\n",
    "    r\"\"\"Convert iterable-style dataset to map-style dataset.\n",
    "\n",
    "    args:\n",
    "        iter_data: An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> from torchtext.datasets import IMDB\n",
    "        >>> from torchtext.data import to_map_style_dataset\n",
    "        >>> train_iter = IMDB(split='train')\n",
    "        >>> train_dataset = to_map_style_dataset(train_iter)\n",
    "        >>> file_name = '.data/EnWik9/enwik9'\n",
    "        >>> data_iter = to_map_style_dataset(open(file_name,'r'))\n",
    "    \"\"\"\n",
    "\n",
    "    # Inner class to convert iterable-style to map-style dataset\n",
    "    class _MapStyleDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, iter_data) -> None:\n",
    "            # TODO Avoid list issue #1296\n",
    "            self._data = list(iter_data)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self._data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data[idx]\n",
    "\n",
    "    return _MapStyleDataset(iter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "if input is 2D, then offsets has to be None, as input is treated is a mini-batch of fixed length sequences. However, found offsets of type <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     35\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 36\u001b[0m     train(train_dataloader)\n\u001b[1;32m     37\u001b[0m     accu_val \u001b[38;5;241m=\u001b[39m evaluate(valid_dataloader)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_accu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m total_accu \u001b[38;5;241m>\u001b[39m accu_val:\n",
      "Cell \u001b[0;32mIn[88], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (label, text, offsets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     predictied_label \u001b[38;5;241m=\u001b[39m model(text, offsets)\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictied_label, label)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/learnpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_full_backward_pre_hook\u001b[39m(\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1164\u001b[0m     hook: Callable[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, _grad_t], Union[\u001b[38;5;28;01mNone\u001b[39;00m, _grad_t]],\n\u001b[1;32m   1165\u001b[0m     prepend: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Registers a backward pre-hook on the module.\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03m    The hook will be called every time the gradients for the module are computed.\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m    The hook should have the following signature::\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m        hook(module, grad_output) -> tuple[Tensor] or None\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m    The :attr:`grad_output` is a tuple. The hook should\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m    not modify its arguments, but it can optionally return a new gradient with\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    respect to the output that will be used in place of :attr:`grad_output` in\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03m    subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03m    all non-Tensor arguments.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[1;32m   1180\u001b[0m \u001b[38;5;124;03m    For technical reasons, when this hook is applied to a Module, its forward function will\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;124;03m    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;124;03m    of each Tensor returned by the Module's forward function.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m    .. warning ::\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m        Modifying inputs inplace is not allowed when using backward hooks and\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03m        will raise an error.\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m        hook (Callable): The user-defined hook to be registered.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m        prepend (bool): If true, the provided ``hook`` will be fired before\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m            all existing ``backward_pre`` hooks on this\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m            :class:`torch.nn.modules.Module`. Otherwise, the provided\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m            ``hook`` will be fired after all existing ``backward_pre`` hooks\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;124;03m            on this :class:`torch.nn.modules.Module`. Note that global\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m            ``backward_pre`` hooks registered with\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m            :func:`register_module_full_backward_pre_hook` will fire before\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;124;03m            all hooks registered by this method.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124;03m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m     handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks)\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n",
      "Cell \u001b[0;32mIn[85], line 16\u001b[0m, in \u001b[0;36mQuestionClassifier.forward\u001b[0;34m(self, text, offsets)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, offsets):\n\u001b[0;32m---> 16\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(text, offsets)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(embedded)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_full_backward_pre_hook\u001b[39m(\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1164\u001b[0m     hook: Callable[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, _grad_t], Union[\u001b[38;5;28;01mNone\u001b[39;00m, _grad_t]],\n\u001b[1;32m   1165\u001b[0m     prepend: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Registers a backward pre-hook on the module.\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03m    The hook will be called every time the gradients for the module are computed.\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m    The hook should have the following signature::\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m        hook(module, grad_output) -> tuple[Tensor] or None\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m    The :attr:`grad_output` is a tuple. The hook should\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m    not modify its arguments, but it can optionally return a new gradient with\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    respect to the output that will be used in place of :attr:`grad_output` in\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03m    subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03m    all non-Tensor arguments.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[1;32m   1180\u001b[0m \u001b[38;5;124;03m    For technical reasons, when this hook is applied to a Module, its forward function will\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;124;03m    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;124;03m    of each Tensor returned by the Module's forward function.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m    .. warning ::\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m        Modifying inputs inplace is not allowed when using backward hooks and\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03m        will raise an error.\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m        hook (Callable): The user-defined hook to be registered.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m        prepend (bool): If true, the provided ``hook`` will be fired before\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m            all existing ``backward_pre`` hooks on this\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m            :class:`torch.nn.modules.Module`. Otherwise, the provided\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m            ``hook`` will be fired after all existing ``backward_pre`` hooks\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;124;03m            on this :class:`torch.nn.modules.Module`. Note that global\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m            ``backward_pre`` hooks registered with\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m            :func:`register_module_full_backward_pre_hook` will fire before\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;124;03m            all hooks registered by this method.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124;03m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m     handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks)\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n",
      "File \u001b[0;32m~/anaconda3/envs/learnpytorch/lib/python3.11/site-packages/torch/nn/modules/sparse.py:387\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input, offsets, per_sample_weights)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, offsets: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, per_sample_weights: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of EmbeddingBag.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m        input (Tensor): Tensor containing bags of indices into the embedding matrix.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m        offsets (Tensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m            the starting index position of each bag (sequence) in :attr:`input`.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m            to indicate all weights should be taken to be ``1``. If specified, :attr:`per_sample_weights`\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m            must have exactly the same shape as input and is treated as having the same\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m            :attr:`offsets`, if those are not ``None``. Only supported for ``mode='sum'``.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m        Tensor output shape of `(B, embedding_dim)`.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m        A few notes about ``input`` and ``offsets``:\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m        - :attr:`input` and :attr:`offsets` have to be of the same type, either int or long\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m        - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m          each of fixed length ``N``, and this will return ``B`` values aggregated in a way\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m          depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m        - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m          multiple bags (sequences).  :attr:`offsets` is required to be a 1D tensor containing the\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m          starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets` of shape `(B)`,\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m          :attr:`input` will be viewed as having ``B`` bags. Empty bags (i.e., having 0-length) will have\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;124;03m          returned vectors filled by zeros.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding_bag(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, offsets,\n\u001b[1;32m    390\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    391\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    392\u001b[0m                            per_sample_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_last_offset,\n\u001b[1;32m    393\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnpytorch/lib/python3.11/site-packages/torch/nn/functional.py:2344\u001b[0m, in \u001b[0;36membedding_bag\u001b[0;34m(input, weight, offsets, max_norm, norm_type, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   2343\u001b[0m         type_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(offsets))\n\u001b[0;32m-> 2344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif input is 2D, then offsets has to be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, as input is treated is a mini-batch of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m fixed length sequences. However, found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2348\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffsets of type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(type_str)\n\u001b[1;32m   2349\u001b[0m     )\n\u001b[1;32m   2350\u001b[0m offsets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnumel(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: if input is 2D, then offsets has to be None, as input is treated is a mini-batch of fixed length sequences. However, found offsets of type <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "LR = 5\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.1)\n",
    "total_accu = None\n",
    "train_iter = ((row['target'], row['question_text'])\n",
    "              for _, row in train_csv.drop(columns = [\"qid\"]).iterrows())\n",
    "test_iter = ((row['target'], row['question_text'])\n",
    "             for _, row in test_csv.drop(columns = [\"qid\"]).iterrows())\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size = BATCH_SIZE, \n",
    "    shuffle = True, collate_fn = collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size = BATCH_SIZE, \n",
    "    shuffle = True, collate_fn = collate_batch\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    test_dataset, batch_size = BATCH_SIZE, \n",
    "    shuffle = True, collate_fn = collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
